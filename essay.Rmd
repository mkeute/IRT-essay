---
title: "Item Response Theory - Final Essay"
date: \today
author: "Marius Keute"
output: bookdown::pdf_document2
bibliography: refs.bib
fontsize: 11pt
github-repo: mkeute/IRT-essay
geometry: margin=2.5cm
---



\newpage

**submitted to:**

**Dr. Stefano Noventa**

**University of TÃ¼bingen**\
\
\

**submitted by:**

**Marius Keute (QDS, 5991873)**

\
\
\

**Statutory Declaration:** I hereby declare that I composed the present paper independently and that I have used no other resources than those indicated. The text passages which are taken from other works in wording or meaning have been identified as such. I also declare that this work has not been partly or completely used in another examination.
\
\
\
\
\
**The full `R` and Markdown code used for generating this essay is available on Github:**

https://github.com/mkeute/IRT-essay


```{=tex}
\newpage
\newpage
```
# Introduction

Understanding sexual habits and behavior can be important for, e.g., improving sex education for adolescents, preventing sexually transmitted diseases (STDs), and identifying high-risk populations for sexual misconduct. The Sexual Compulsivity Scale (SCS) is a 10-item questionnaire constructed to measure hypersexuality and high libido in a given person (@kalichman1995sexual,@kalichman2001sexual). Each of the 10 items is a statement about sexual habits, feelings, or experiences, and the test-taker can indicate how much they can relate to each statement on a four-level scale ranging from 1 (Not at all like me) to 4 (Very much like me).

The 10 items are (@kalichman2001sexual):

Q1.  My sexual appetite has gotten in the way of my relationships.

Q2.  My sexual thoughts and behaviors are causing problems in my life.

Q3.  My desires to have sex have disrupted my daily life.

Q4.  I sometimes fail to meet my commitments and responsibilities because of my sexual behaviors.

Q5.  I sometimes get so horny I could lose control.

Q6.  I find myself thinking about sex while at work.

Q7.  I feel that sexual thoughts and feelings are stronger than I am.

Q8.  I have to struggle to control my sexual thoughts and behavior.

Q9.  I think about sex more than I would like to.

Q10. It has been difficult for me to find sex partners who desire having sex as much as I want to.

In this essay, using data from the original validation cohort (@kalichman2001sexual), I will provide a thorough analysis of the SCS, using methods derived from Item Response Theory (IRT), and to a lesser extent from Classical Test Theory (CTT). In the final section, I will give an overview over both theories and their key differences.

\newpage

# Preparing the Data

The dataset (@kalichman1995sexual, available at http://openpsychometrics.org/_rawdata/SCS.zip) consists of 3376 observations, the variables being the ten items of the SCS, the sum score, gender and age. From the age variable, three cases where the reported age was 100 years or higher appeared implausible and therefore set to missing values. The remaining cases had a mean age of 30.9 years (median 28 years, range [14, 85]). From the gender variable, 13 values were missing and 15 cases where the reported gender was "3" (other) were set to missing values. Of the remaining cases, 2295 (68.5%) reported male gender ("1") and 1053 (31.4%) reported female gender ("2"). In the dataset, 133 cases contained at least one missing value.

The pattern of missing SCS items is shown in Figure \@ref(fig:missing-plot). It can be seen that item Q9 was missing most often, though not by a large margin (Q9: 27 missing values, Q5: 13 missing values). It can be seen that the majority of cases with missing values (118 cases / 88.7%) had only a single missing item, while there were no prominent patterns of items that tended to be jointly missing. Eight cases where more than two SCS items were missing were excluded from all further analyses. For the remaining 3368 cases, the probability of missing values at each SCS variable was modeled as a function of the values in *all other* SCS variables using a logistic regression model:

$P(M_{i,q} = 1|X_{i,\not q}) = \sigma(X_{i,\not q} \hat{\beta}_{\not q})$,

where $M_{i,q}$ is 1 if the $i^{th}$ person has a missing value at item $q \in \{Q1, Q2, ... Q10\}$ and 0 otherwise, $X_{i,\not q}$ denotes the item values of all other items except item $q$, $\sigma$ is the logistic function $\sigma(x) = \frac{1}{1-e^{-x}}$, and $\hat{\beta}_{\not q}$ are the estimated regression weights based on all other items (@guan2011missing). Note that each variable's pattern of missing values could only be predicted based on the observations without missing values in any other variable, since cases with any missing values were excluded by the logistic model by default of the implementation. Since the majority of cases had either no or only one variable missing, however, this should not bias the overall picture very much.

```{r missing-plot,  echo=FALSE, out.width="70%", fig.align = 'center', fig.cap="Pattern of missing SCS values."}
knitr::include_graphics("missingplot.pdf")
```

\newpage

# Descriptive Analyses and Dichotomization

The distribution of responses for each item before dichotomization can be seen in Figure \@ref(fig:distroplot). All item categories show reasonable coverage of the range of responses (1-4), and there are no obvious flooring or ceiling effects, except for a potential ceiling tendency with item Q6 (few cases with response 1, many with response 4).

```{r distroplot, echo=FALSE, out.width="70%", fig.align = 'center', fig.cap="Distribution of non-dichotomized responses per item"}
knitr::include_graphics("distroplot.pdf")
```

For dichotomization of the item data, I considered two options, namely, thresholding each of the 10 items at its own median, to ensure an even distribution of observations into both categories for each item, or finding a common threshold for all items. Since the items have only four levels each, a median split would not necessarily lead to a very balanced dichotomization. Furthermore, the item levels are designed to have the same meaning across all items, therefore I decided to dichotomize at a common threshold of 2, i.e., the dichotomous items $D_q \in \{D_1, D_2, ..., D_{10}\}$ were defined such that

```{=latex}
$$
D_{i,q} = \begin{cases}
0 \text{ if } Q_{i,q} \in \{1,2\},\\
1 \text{ if } Q_{i,q} \in \{3,4\},\\

\end{cases}
$$
```
Of note, simple models in IRT such as the Rasch model (see below) assume that all item responses are either correct or incorrect (or solved / unsolved, respectively). Since a personality test such as the SCS does not have right or wrong responses, it is common to dichotomize the values, as described above, and henceforth treat one of the dichotomous response options as the 'correct' one, in this case, responses greater than 2. This is, however, purely for compliance with IRT terminology and does not imply that the 'correct' dichotomous responses are better than the 'incorrect' ones in any way. Likewise, I will refer to the latent person scores that the model estimates as 'ability', again for terminological compliance, while they really do not indicate an ability but rather the sexual compulsivity trait that the SCS is supposed to measure.

Descriptive characteristics of the 10 SCS items are shown in Table \@ref(tab:descriptivestab), the proportions of correct responses are shown in Figure \@ref(tab:dichdistrotab). Since most variables' median was 2, this was not much different from an item-wise median threshold (see Table \@ref(tab:descriptivestab)).

 Moreover, I calculated item discrimination, i.e., each items ability to discriminate between high- and low-scoring individuals, using the adjusted item-total correlation method (@reynolds2021mastering), i.e., by calculating biserial correlation coefficients between each (dichotomized) item's scores and the sum of all other (dichotomized) items.

Tetrachoric intercorrelations of the (dichotomized) items are shown in Figure \@ref(fig:biserialcorplot). It can be seen that all pairs of items show moderate to high positive correlations, indicating that all items measure similar information yet are not redundant (see below for further scrutiny of factorial structure). Item easiness (i.e., proportion of correct responses) was between 27% (item Q4) and 71.9% (item Q6), item discrimination was between .26 (item Q6) and .45 (items Q1, Q2), i.e., there was no item with a trivial response pattern (e.g., all or no responses correct), and no item was, in and of itself, a very good representation of the entire scale, since all item discriminations were only moderate in size.

```{r descriptivestab, echo=FALSE}
descriptives=read.csv("descriptives.csv")
knitr::kable(descriptives, caption = "Descriptive item statistics (mean, median and range *before* dichotomization)")
```

```{r dichdistrotab, echo=FALSE}
dich.stats = read.csv("dich_stats.csv")
kableExtra::kable_styling(knitr::kable(dich.stats, caption = "Distribution and discrimination of dichotomized items"),latex_options = c("striped", "scale_down"))


```

```{r biserialcorplot, echo=FALSE, out.width="80%", fig.align = 'center', fig.cap="Tetrachoric intercorrelations between items."}
knitr::include_graphics("tetrachoric_cor_mat.pdf")
```

\newpage

# Rasch models

After analyzing the SCS data using descriptive statistics and concepts derived from CTT, in the following I will fit and discuss different IRT models to the data.

## Rasch model estimation

Next, I estimated a Rasch model for the SCS data, also known as either the one-parameter logistic model or one-parameter normal ogive model, depending on the parameterization.

It models a given person's chances of solving a given item as a logistic function of the difference between the $q^{th}$ item's difficulty $\beta_q$ and the $i^{th}$ person's ability $\theta_i$, where $\beta_q$ and $\theta_i$ are latent (unobserved) quantities that are estimated from the dichotomous (solved vs. not solved) item data.

The probability for a given person can then be expressed by the logistic function: $P(D_{i,q} = 1 | \beta_q, \theta_i) = \sigma(\theta_i - \beta_q)$, where $\sigma$ is the logistic function as specified above. That is to say, it is purely the difference between item difficulty and person ability that explains the correctness of item responses within the model.

Crucially, Rasch model assumes that this relationship is identical for all items, i.e., the logistic function can only be shifted in threshold, but not changed in slope across items with different difficulty. Item difficulty is, therefore, the only free parameter of the Rasch model, whereas alternative models (see below) also estimate additional parameters.

To obtain a comprehensive picture, I fitted Rasch models using three different software implementations in `R` 4.1.

The first method was the one implemented in the R package `eRm` (@mair2007extended). The `eRm::RM` function estimates a Rasch model using conditional maximum likelihood estimation. To make the model identifiable, the user can choose between two model constraints, namely that the model parameters must sum to 0 or that the first item's parameter is fixed to 0. I chose the first (default) option, i.e., forcing item difficulties to sum to 0. Item discriminativity, i.e., the steepest slope of the logistic functions (at $\beta_q = \theta_i$), is fixed to 1 for all items in this implementation.

The second method was the one implemented in the R package `ltm` (@rizopoulos2006ltm). The `ltm::rasch` function estimates a Rasch model using approximate marginal maximum likelihood estimation. This package provides the user with more flexibility to impose constraints on the model than `eRm`, I fixed item discriminativity to 1 for all items, to maximize comparability with the `eRm` parameters.

The third method was a structural equation model as implemented in `lavaan` (@rosseel2012lavaan). Unlike the two previous implementations, `lavaan` requires a more explicitly user-defined model specification, as it does not provide any ready-made function or syntax for Rasch models.

I used a modified copy of the syntax presented in @templin2022IRT:

    SCS =~ 1*Q1 + 1*Q2 + 1*Q3 + 1*Q4 + 1*Q5 + 1*Q6 + 1*Q7 + 1*Q8 + 1*Q9 + 1*Q10

    Q1 | t1; Q2 | t1; Q3 | t1; Q4 | t1; Q5 | t1; Q6 | t1; Q7 | t1; 
    Q8 | t1; Q9 | t1;Q10 | t1;

    SCS ~ 0;

    SCS ~~ 1*SCS

Again, I fixed item discriminativities to 1 for all items. The item parameters `Q1`, ..., `Q10` were subjected to a common threshold `t1`, and the sum of all item parameters (corresponding to the latent variable `SCS`) was fixed to 0, as with `eRm`. Moreover, its variance was fixed to unit. Of note, due to limitations of the implementation, `lavaan` is not able to estimate Rasch models using maximum likelihood estimation, but only using mean- and variance-adjusted weighted least squares (WLSMV) estimation, which limits model fit comparisons. The `lavaan` model did not give back IRT-compatible difficulty ($\beta$) coefficients immediately, but they had to be calculated by dividing the items estimated `tau` coefficients by the respective `lambda` coefficients.

## Model analysis

The item difficulty parameters of the three models are shown in Figure \@ref(fig:difficultyplot), along with the item difficulty derived from CTT (i.e., the proportion of incorrect responses per item in the data). While the parameters differed between the different models, it is important to note that the parameters from all four models (including CTT) were perfectly correlated for all pairs of models (all r \> .999), which indicates that the parameters of one model are simply affine linear transformations of the parameters of any other model, i.e., while numerically different, the models incorporated identical information about the items. The corresponding item-characteristic curves (ICC) are shown in Figure \@ref(fig:iccplot). ICCs are generated by calculating the function graph of the item-wise logistic functions parameterized by item difficulty, across a range of possible person ability values on the x-axis.

```{r difficultyplot, echo=FALSE, out.width="70%", fig.align = 'center', fig.cap="Item difficulties in comparison."}
knitr::include_graphics("diffcfig.pdf")
```

```{r iccplot, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Item-characteristic curves for the three Rasch models."}
knitr::include_graphics("iccfig.pdf")
```

The overall likelihood-based model fit indices are shown in Table\@ref(tab:modelfittab). Of note, log-likelihood and information criteria can only be reported for those models fitted using `ltm` and `eRm`, while the `lavaan` model's fit indices are not comparable, as it was not fitted using maximum likelihood estimation. For brevity, I skip the discussion of the `lavaan` model fit indices. Comparing the fit indices for the `ltm` and `eRm` Rasch models, it can be seen that `eRm` had an overall higher log-likelihood. Since it also had one free parameter less (because of the sum constraint, see above), it was, overall, the preferred model also according to the Akaike and Bayes-Schwarz information criteria.

```{r modelfittab, echo=FALSE}
rasch_model_fits = read.csv("rasch_model_fits.csv")
knitr::kable(rasch_model_fits, caption = "Fit indices for Rasch models")
```

Finally, to get an impression of how the three models perform for each item, I calculated the mean 0-1-loss per item, compared to the actual, dichotomized data $D_{i,q}$, as:

```{=latex}
$\mathcal{L}_{q} = \frac{1}{n} \sum_{i=1}^n |f(\theta_i,\beta_q) - D_{i,q}|$
```
that is to say, I used the item difficulty parameters $\beta_q$ and person ability scores $\theta_i$ estimated by the models to predict the expected response for each person and item:

```{=latex}
$$
f(\theta_i,\beta_q) =: \begin{cases}
1 \text{ if } \sigma(\theta_i-\beta_q) > 0.5,\\
0 \text{ otherwise }\\

\end{cases}
$$
```
The mean loss is then calculated as the proportion of incorrectly predicted cases for each item. It is shown in Figure \@ref(fig:itemlossplot). It can be seen that the three models, despite differences in parameterization, performed very similarly, with the `eRm` and `lavaan` models performing almost identically, whereas the `ltm` model tended to incur a slightly higher loss, except for items Q6 and Q10. Interestingly, these are the most difficult items, and the fact that `ltm` outperformed `eRm` especially for those items might be related to the differences between conditional vs. marginal maximum likelihood estimation, which have the strongest effect in cases where either all or no responses are correct, i.e., for particularly easy or difficult items.

```{r itemlossplot, echo=FALSE, out.width="70%", fig.align = 'center', fig.cap="Mean 0-1-loss (proportion of incorrectly predicted responses) per item."}
knitr::include_graphics("itemlossfig.pdf")
```

## Differential Item Functioning

I tested for differential item functioning (DIF) using the package `difR` and the procedure outlined in the companion paper (@magis2010general). DIF is a disadvantageous property of a Rasch model, meaning that item responses differ between subjects from different participant groups, even given the same estimated ability level. The presence of DIF indicates a lack of measurement invariance of the model. The results are displayed in \@ref(fig:diffig). I used the `difLord` method to investigate DIF, but obtained essentially the same results with the `difRaju` method. I discarded `difLRT`, the third recommended IRT-related method, due to its high computational demand. I tested for DIF across genders (male vs. female) and age groups (above median age vs. smaller or equal to median age). Significant DIF (FDR-corrected p-value \< .05) across the gender groups was detected for items Q5 and Q10, and across the age groups for item Q1, Q5, and Q10. However, the effect sizes were in the negligible range for all but item Q5 in the gender group comparison, where a moderate effect was detected ($\Delta_{Lord} = -1.14$). Item Q5 reads 'I sometimes get so horny I could lose control', and it appears plausible that DIF for this item could emerge by gender-specific societal norms, and possibly gender-specific consequences of 'being horny' for everyday functioning. Since I have only been considering Rasch (one-parameter) models so far, this analysis only tested for uniform DIF.

```{r diffig, echo=FALSE, out.width="70%", fig.align = 'center', fig.cap="p-values (FDR corrected, negative log-transformed) from DIF analysis. Black line: significance threshold (p < .05)"}
knitr::include_graphics("DIF_pvals.pdf")
```


# Higher-parameterized IRT models

There are several extensions and alternatives to the Rasch model with its restrictive assumptions that differences between items can be described by just one parameter, namely item difficulty, while the Birnbaum model or 2-parameters logistic model also takes into account item discriminativity (corresponding to varying slopes of the item-characteristic curves of different items), and other possible models additionally include a guessing probability term (corresponding to various vertical offsets of the item-characteristic curves of different items) or ceiling probability term (corresponding to clipping the item-characteristic curves from above). For the given dataset, it appears reasonable to estimate a Birnbaum model, whereas 3-PL or 4-PL models seem difficult, since guessing and ceiling probabilities are not easy to operationalize for the dataset, considering that there is no ground truth to the items and we found no prominent ceiling or flooring in any item.

For fitting the 2-PL model, I used the `ltm::ltm` function, and the Rasch model fitted using `ltm::rasch` served as a baseline model for comparison. ICCs and estimated item difficulty parameters are shown in Figure \@ref(fig:ICC2PLfig) and \@ref(fig:diffc2PLfig), respectively. It can clearly be seen that item discriminativities, and with them the slopes of the ICC curves, vary considerably between items. A side-effect of two-parameter modeling is that the ICCs now cross each other, i.e., there is no clear order of difficulty between items anymore, but whether one item is more difficult than another can now be dependent on the person ability.

Comparing the two models using a Likelihood Ratio Test, I found the 2-PL model to fit the data significantly better than the Rasch model (log-LR = 1741.55, p < .001).

```{r ICC2PLfig, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Item-Characteristic Curves from 2-PL model"}
knitr::include_graphics("ICC_2PL.pdf")
```
```{r diffc2PLfig, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Estimated item difficulties and discriminativities based on CTT, Rasch model and 2-PL model. For both IRT models, error bars indicate standard errors."}
knitr::include_graphics("difficulties_plot_2PL.pdf")
```


For further model comparison, I calculated infit and outfit indices for each item and model. Infit and outfit indices are based on model residuals. While outfit (short for outlier-sensitive fit statistic) is particularly sensitive to unexpected responses in cases where item difficulty and person ability are far apart (e.g., a low-ability person unexpectedly solves several very difficult items), infit (short for information-weighted fit statistic) is particularly sensitive to unexpected responses in cases where item difficulty and person ability match (e.g., a person solves far less or far more than half of the items whose difficulty equals their ability). Both are based on the normalized residuals $Z_{iq} = \frac{D_{iq}-\mathbb{E}(D_{iq})}{\sqrt{var(D_{iq})}}$, where $D_{iq}$ is the actual response of person i to item q, $\mathbb{E}(D_{iq}) = P(D_{iq}=1|\beta_q,\theta_i)$ is the conditional expectation for this person's response to the item, given the model parameters, calculated using the logistic function as shown above. $var(D_{iq})$ can be calculated as $P(D_{iq}=1|\beta_q,\theta_i)(1-P(D_{iq}=1|\beta_q,\theta_i))$. The infit index for item q is then defined as: $Infit_q = \sum_{i=1}^n (\frac{var(D_{iq})}{\sum_{i=1}^n var(D_{iq})}Z_{iq}^2)$, the outfit index is defined as: $Outfit_q = \sum_{i=1}^n \frac{Z_{iq}^2}{n}$. For both indices, values close to 1 indicate good fit, whereas higher values indicate under- and lower values overfitting.

The infit and outfit values for both models are, for the most part, in the acceptable (\>0.7 and \<1.3) range (see Figure \@ref(fig:inoutfit2PLfig)). For infit, the 2-PL model consistently outperforms the Rasch model, whereas the outfit values tend to be lower overall, and both models are much closer to each other.


```{r inoutfit2PLfig, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Infit and outfit indices for each item in the Rasch and 2-PL models"}
knitr::include_graphics("inoutfit_plot_2PL.pdf")
```



# Polytomous IRT model

Next, I went back to the original, non-dichotomized data and fitted a polytomous IRT model to it, i.e., a model that accounts for more than two response categories per item and therefore takes the full richness of the data into account. The first step in polytomous IRT modeling is choosing the appropriate model class, depending on the structure of the response alternatives. In the case of the SCS, responses are ordered, ranging from 1 to 4, indicating monotonously increasing degrees of agreement with the item (or correctness, in IRT terminology), therefore a graded response model (GRM) seemed appropriate. I used the `ltm::grm` function, following the procedure outlined in @smyth2022Item. 

The first decision to be made in the GRM was whether to use an unconstrained or constrained model, i.e., whether or not to allow item discriminativity to vary between items. I found that an unconstrained model was a significantly better fit to the data (LRT = 821.42, p<.001), analogously to the dichotomized data models, where the 2-PL model with varying item discriminativities was also preferred over the Rasch model.

Exemplary ICCs for item Q2, i.e., the item with the highest discriminativity in the dichotomized case, are shown in Figure \@ref(fig:polyICC)). There is now one ICC curve for each response category, while the y-axis still gives the probability for a person with a given latent ability level to give a response in a certain category. One can see nicely that the response categories follow the monotonous order that is intended by design.

```{r polyICC, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Item-Characteristic curves from polytomous GRM for item Q2"}
knitr::include_graphics("ICC_polytomous_itemQ2.pdf")
```


Additionally, Item Information Curves can be generated, as shown in Figure \@ref(fig:polyIIC)). The figure contains one IIC curve per item, indicating how informative each item is with respect to the overall sum score as a function of the latent person ability. It is, therefore, roughly equivalent to item discriminativity, additionally resolved by person ability levels. 
It can be seen that the majority of items are most informative at a medium level of person ability, and that items Q8, Q3, Q7, and Q2, are the most informative items in the medium ability range. Item Q6, on the other hand, tends to be relatively informative also in the lower ability range, while item Q4 extends its region of high informativity slightly higher into the high-ability range than the other items.

```{r polyIIC, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Item Information curves from polytomous GRM for item Q2"}
knitr::include_graphics("IIC_polytomous.pdf")
```

# Factor models

Following the IRT analysis of the dichotomized data, I went back to the original, non-dichotomized data, to investigate its factorial structure. It has been suggested that the SCS can best be described by two latent factors, one comprising items Q1, Q2, Q3, Q4, and Q10, being related to consequences of sexual behavior and compulsivity to one's lifestyle, and a second one comprising items Q5, Q6, Q7, Q8, and Q9, being related to the compulsivity of one's sexual thoughts without necessarily affecting actual behavior. Using `lavaan::cfa`, I fitted several confirmatory factor analysis (CFA) models to the data to find the latent structure that describes the data best. I specified four candidate latent structures:

The first candidate structure was a unidimensional model, i.e., the data can be explained by a single underlying latent factor.

The second candidate structure was a two-factor correlated-traits model, i.e., two latent factors were specified with item loadings as described above, and correlations between the two latent factors were allowed.

The third candidate structure was a bifactor model, i.e., two latent factors were specified with item loadings as described above, with an additional general factor on which all items load. The general factor was constrained to be orthogonal to the subfactors.

The final candidate structure was a hierarchical factor model, which specifies the two item-specific factors, and additionally has them load on a shared second-order factor.

Models were fitted with standardized latent variables, i.e., the variance of all latent factors was fixed to unit. The models were specified in `lavaan` syntax as follows:

    Unidimensional model:
      xi1 =~ Q1+Q2+Q3+Q4+Q5+Q6+Q7+Q8+Q9+Q10
      
    Correlated-traits model:
      xi1 =~ Q1+Q2+Q3+Q4+Q10
      xi2 =~ Q5+Q6+Q7+Q8+Q9

    Bifactor model:  
      G =~ Q1+Q2+Q3+Q4+Q5+Q6+Q7+Q8+Q9+Q10
      xi1 =~ Q1+Q2+Q3+Q4+Q10
      xi2 =~ Q5+Q6+Q7+Q8+Q9
      G ~~ 0*xi1
      G ~~ 0*xi2
      
    Hierarchical model:
      xi1 =~ Q1+Q2+Q3+Q4+Q10
      xi2 =~ Q5+Q6+Q7+Q8+Q9
      G =~ xi1+xi2


The comparison of fits between the four factor models is shown in \@ref(tab:cfamodelfittab). Models were compared with respect to the Akaike (AIC) and Bayes-Schwarz (BIC) information criteria. Among the four candidate models, the bifactor model was clearly the preferred one among the four candidate models according to AIC as well as BIC.

```{r cfamodelfittab, echo=FALSE}
cfaaov_df=read.csv("cfaaov_df.csv")
knitr::kable(cfaaov_df, caption = "Model comparison between CFA models")
```

The 'winning' bifactor model is illustrated in Figure \@ref(fig:semplotbifactor). Obviously, the fact that the bifactor model is the preferred option among the four candidate models does not mean that it is necessarily a good description of the data in an absolute sense. To understand the absolute goodness-of-fit (not just compared to other models), there is a range of fit indices that we can consider. In particular, the root mean squared error of approximation (RMSEA), standardized root mean squared residual (SRMR), comparative fit index (CFI), and Tucker-Lewis index (TLI) are informative. For the bifactor model, RMSEA was at 0.073 (RMSEA < 0.08 indicating acceptable, RMSEA < 0.05 indicating good fit by convention), SRMR was at 0.028 (SRMR < 0.05 indicating good fit by convention), CFI was at 0.973 (CFI > 0.95 indicating good fit by convention), and TLI was at 0.95 (TLI > 0.95 indicating acceptable, TLI > 0.97 indicating good fit by convention). Overall, the bifactor model was, therefore, an acceptable to good fit for the SCS data.

```{r semplotbifactor, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Factor structure and loadings of bifactor model"}
knitr::include_graphics("semplot_bifactor.pdf")
```

An open question with respect to the factorial structure is to which subscale item Q10 should belong. While it has been assigned to the first subfactor, its loading on the factor is low (0.18, around half as high as the second-lowest loading item, Q4). To investigate the issue, I fitted two alternative bifactor models, one (Alternative 1) where item Q10 belonged to the second subfactor, together with items Q5 - Q9, and one (Alternative 2) where item Q10 constituted its own, third subfactor. Additionally, I used the `psych::omega` function to fit a bifactor model with automatized identification of the factor structure. This procedure also returned a three-factor structure, where the first four items loaded on one subfactor, items Q5 - Q9 loaded on another subfactor, and items Q1, Q6, and Q10 loaded on a third subfactor (see Figure \@ref(fig:semplotbifactorautomatic)). I refitted this model in `lavaan` to facilitate comparisons.

```{r semplotbifactorautomatic, echo=FALSE, out.width="100%", fig.align = 'center', fig.cap="Factor structure and loadings of automatically identified bifactor model"}
knitr::include_graphics("semplot_bifactor_automatic.pdf")
```

Considering all fit indices reported above (see Table \@ref(tab:bifactorcomparison)), the original factor structure was clearly preferred over the Alternative 1 bifactor model, but the Alternative 2 bifactor model was a close competitor to the original bifactor model, that is to say, item Q10 arguably constitutes a third subfactor on its own. The automatically identified factor structure, however, was clearly preferred over the original model by all fit indices. 

Looking at the content of the items (see Introduction), we can see that item Q10 is the only item that explicitly involves sex partners and difficulties to find sex partners, while item Q1 mentions relationships, item Q6 mentions the work environment, but all other items do not explicitly refer to relationships with other persons. It could, therefore, be, that responses to those items are not only influenced by sexual compulsivity, but also by a range of social and communicative abilities that might influence whether someone has difficulties finding sex partners and getting along with relationship partners and coworkers or not. Therefore, it appears plausible that those three items apparently form a third subfactor.



```{r bifactorcomparison, echo=FALSE}
bifactor_comparison=read.csv("bifactor_comparison.csv")
knitr::kable(bifactor_comparison, caption = "Model comparison between different bifactor models")
```

# Reliability and Unidimensionality

As outlined above, the SCS is better described by a bifactor model with two or three subfactors than by a fully unidimensional model. It can, therefore, be concluded, that the scale is not unidimensional. To determine the composite reliability of the scales with respect to its subscales determined by confirmatory factor analysis, I calculated composite reliability using `semTools::reliability`. Composite reliability is a measure of internal consistency and is related to the question of whether item sum scores are a good representation of the underlying, latent quantity.

The function gives back Cronbach's $\alpha$ as well as the $\omega_1$, $\omega_2$, and $\omega_3$ coefficients both for the original bifactor model with two subfactors (see Figure \@ref(fig:semplotbifactor)) and for the better-fitting, but more complex bifactor structure with three subfactors (see Figure \@ref(fig:semplotbifactorautomatic)). The results are shown in Table \@ref(tab:compositereltab). 

```{r compositereltab, echo=FALSE}
compositerel=read.csv("composite_reliability.csv")
knitr::kable(compositerel, caption = "Composite reliability scores for the bifactor models with two and three subfactors") 
```
It can be seen that the estimated reliability differs substantially between scores, and between models. Unsurprisingly, the third subfactor $\xi_3$ from the three-factor model has relatively low reliability, which might be related to the fact that it is based on fewer items. The remaining factors G, $\xi_1$ and $\xi_2$ have acceptable to good composite reliability in both models according to Cronbach's $\alpha$ and $\omega$, ranging from 0.71 to 0.90. Differences between scores are related to the way those coefficients are calculated. All three variants of $\omega$ are calculated as a fraction, with the squared sum of loadings of a given factor, multiplied with the factor variance (which is 1 in our models) in the numerator. What differs is the denominator. Different variants of $\omega$ are calculated with denominators based on either the residual variance, empirical covariance, or model-implied covariance. The fact that $\omega_3$ is consistently smaller than $\alpha$ could point to relatively equal general factor loadings.


# Measurement Invariance

As a last step in data analysis, I investigated measurement invariance of the SCS data. Measurement invariance means that the factorial structure is independent of other variables that are not part of the structure. In particular, I tested whether or not the three-subfactor bifactorial structure was identical across genders and age groups, similar to the analysis of DIF (see above). Following the procedure outlined in @xu2012multiple and @van2012checklist, I compared four models with increasingly strict measurement invariance assumptions. The *first* model assumes *configural invariance*, i.e., model parameters can vary freely in each group, and no assumption about invariance between the groups is made. The *second* model assumes *weak invariance* or *metric invariance*, i.e., equal factor loadings across groups. The *third* model assumes *strong invariance* or *scalar invariance*, where factor loadings and item intercepts are assumed to be equal across groups. The final model assumes *strict* invariance, where, additionally, item residual variances are assumed to be equal across groups. Of note, I had to use the two-subfactor bifactorial structure to test for gender measurement invariance, and the three-subfactor bifactorial structure to test for age group measurement invariance, since otherwise, there would have been always one or several models that did not converge.

For gender, the overall preferred model was the one with weak invariance (see Table \@ref(tab:migendertab)), while for age, eithert the model with weak or configural invariance was preferred, depending on whether AIC, BIC, or the significance test were considered. (see Table \@ref(tab:miagetab)). 

```{r migendertab, echo=FALSE}
mi_gender_compout=read.csv("mi_gender_compout.csv")
knitr::kable(mi_gender_compout, caption = "Model comparison for measurement invariance across genders") 
```

```{r miagetab, echo=FALSE}
mi_age_compout=read.csv("mi_age_compout.csv")
knitr::kable(mi_age_compout, caption = "Model comparison for measurement invariance across age groups") 
```

Looking at modification indices of the weak invariance models for age and gender, I found that the suggested modifications for the gender model included allowing factor loadings for items Q1 and Q5 on the latent factors $\xi_1$ and $\xi_2$, respectively, to differ between groups, while for the age model, freeing factor loadings for item Q1, Q5, and Q10 between groups was suggested. This is roughly in agreement with what I found with respect to DIF (see above).

\newpage

# Theoretical Part: Key differences between IRT and CTT

## Introduction

Unlike some physical quantities, many of the variables of interest in psychology, economics, and other human-centric fields, are latent, i.e., not directly observable. Researchers often try to reconstruct such latent variables by combining several observable variables. In particular, for psychological concepts such as personality traits, a person's score will often be estimated as a combination of item responses in a psychological test. Even though forerunners of psychological tests have been around for centuries, a comprehensive theory of psychological testing only emerged roughly half a century ago. Commonly, Melvin Novick is considered the first author to present a comprehensive account of Classical Test Theory (CTT) (@novick1965axioms). Around the same time, a probabilistic view of psychological testing began to emerge, which are now referred to as Item Response Theory (IRT) (@rasch1960studies). It is interesting to note that *Classical* Test Theory does, therefore, not refer to the theory itself being older, but that it rather describes the 'classical' way authors thought about psychological testing from the early $20^{th}$ century onward, whereas probabilistic approaches became popular only later, when increasing computational capacity made them practical. In the following, I will describe some of the core ideas underlying CTT and ITT, their respective strengths and limitations, and practical applications. 

## Core Ideas and Terminology
A test, in the sense CTT as well as IRT use the term, is designed to measure a defined *trait* or *state* of a unit (often a person). The trait/state itself is assumed to be unobservable and is captured by combining several *items* that are thought to be reflective of the trait/state. Items in the test-theoretic sense have defined response categories, which can be either dichotomous (e.g., yes/no or solved/unsolved), or ordered (e.g. I... do not agree / agree a little / fully agree), or multinomial (e.g. my favorite color is... red/blue/green). Multinomial items where the responses can not be ordered, nor dichotomized (e.g., into correct and wrong) are more involved from a theoretical point of view and will not be further discussed here. In the end, all items of a test are usually combined by a linear function, i.e., a weighted sum, to form the test *score*. Any theory of psychological tests is concerned with how test scores come about and how they relate to the actual quantity that the test is supposed to measure.

Tests are described by their *Objectivity*, *Reliability*, and *Validity*. Objectivity is concerned with independence of the test result from the person that administers the test. Reliability means consistency or reproducibility of results, and validity refers to the test being valid, i.e., actually measuring the state or trait it is supposed to measure.

Single items within a test, on the other hand, can be described by their *difficulty* and *discrimination*. Item difficulty is related to the proportion of examinees that solve the item, where *solving* can actually mean to give the correct response if a correct response exists, or could otherwise mean responding 'yes' to a question or responding above a defined threshold if there are more than two response options.

The above terms exist, conceptually, in IRT and CTT alike, even though their mathematical formulation might differ.In the following, I will describe how some of them are defined in CTT and IRT, respectively, and how they are linked to underlying theoretical concepts.

## Assumptions and Problems of CTT
At the heart of CTT is the so-called classical true score model. It states that a given person's test score is an additive combination of the person's *true score* and the *test error*: $X_i = \tau_i + \epsilon_i$. The true score is defined (!) as the expected value $\mathbb{E}X_i$. As a corollary, for the error it holds that $\mathbb{E}\epsilon_i = 0$. CTT assumes that true scores and test errors are uncorrelated within the same test and across tests (the true score of any test is uncorrelated with the error of any other test), and that test errors for the same person across several repetitions of the same test or across different tests are uncorrelated, that is to say, the test is assumed to be equally accurate regardless of the specific examinee and regardless of whether the examinee has a high or low true score (@crocker2008introduction).

Taken together, those assumptions amount to the tenet that the variable of interest is represented by the test scores in an unbiased way, and that only unsystematic test errors prevent tests from representing it perfectly. 

While this theory obviously has some shortcomings, which I will discuss below, it also has advantages. With the above assumptions in place, calculating test-retest reliability, internal consistency, or interrater reliability are straightforward and essentially only require calculating correlations between test scores. Test scores can easily be corrected for reliability-related attenuation, and also item-level characteristics such as difficulty and discrimination can easily be calculated as arithmetic means of responses and correlations between item responses and test scores, respectively. Overall, CTT provides a simple theoretical framework that can prove useful in practice.

However, as mentioned, CTT comes with several downsides. First, it is primarily a theory of test errors, while it only has a very simple notion of how the 'true score' is related to the examinee's actual state/trait and to the single items that the test is made up of. The assumptions of uncorrelated errors are restrictive, since it does not appear plausible that tests are equally accurate irrespective of whether the examinee has a high or low value on the quantity of interest. Finally, CTT provides no method for considering test and person characteristics independently from each other (@linden1997item).

## Strengths and Limitations of IRT 
IRT has been designed to overcome some of the limitations of CTT. It provides a theory of individual *item responses*, rather than starting with the total test score. Moreover, IRT does not have a notion of test errors. Instead, responses to single items of a test are modeled as a *probabilistic* function of the difficulty of the item and the ability (latent trait/state) of the examinee, the so-called *item response function*. If a person with low ability solves an item with high difficulty, this is not thought of as an *error* within IRT, but rather as a less likely, yet not impossible, realization of a random variable. Both item difficulties and person abilities are unknown a priori and are estimated from data using a probabilistic (often logistic) model, as described in more detail in the practical section.

IRT is, overall, a more flexible framework to characterize tests than CTT. For instance, it does not assume tests to be equally accurate for low- and high-ability persons. Rather, the logistic function comes with a natural saturation property that can model items to have different sensitivity to different ability levels. Graphically, this becomes apparent when looking at ICCs (see above) (@crocker2008introduction).

The key difference between CTT and IRT is that IRT explicitly formulates a testable latent variable model that explains the relationship between person abilities and item properties, while CTT simply assumes a linear link between item responses and person abilities (test scores), without providing a proper means to test this assumption. Whithin IRT, the question of whether items can be summed up, i.e., whether items are independent of each other with respect to person ability, and whether the test is unidimensional, can be explicitly tested. Moreover, the item coefficients of an IRT model are, at least in theory, sample-independent, in contrast to CTT.

The notions of reliability, item difficulty, and item discrimination are also defined within the latent variable model in IRT, rather than being calculated from the sample data. Reliability is usually tested in terms of unidimensionality of a factor model, i.e., whether item responses can well be explained by a single underlying latent factor. Item difficulty and discrimination, on the other hand, can be directly read off from the latent variable model. Item difficulty describes the item threshold, i.e., the level of person ability necessary to have a 50% chance of solving an item. Item discrimination, on the other hand, is fixed to be equal across all items in the simplest case (the Rasch model) and is a free parameter only in higher-parameterized models such as the Birnbaum model. It can be thought of as an item's sensitivity to changes in person ability. Graphically, it appears as the slope of the ICC in its steepest point.

The greater flexibility of IRT models comes, on the other hand, with greater responsibility on the part of the human user. IRT models are undoubtedly more difficult to understand and operate, and require more choices (e.g., should a Rasch model with one free parameter, a Birnbaum model with two free parameters, or an even more complex model be used?). Moreover, IRT models come with a set of assumptions about the item response data in order to describe the test appropriately. Assumptions of IRT are, in particular, local independence of items, i.e., mutual conditional independence of item responses given the person ability score, and unidimensionality of the test, and an item response structure that can be described by the item response function, in particular, a monotonic relationship between person ability and probability to solve the item (@crocker2008introduction). 

## Conclusion and Application
I have briefly outlined key tenets, strengths and limitations of CTT, and how IRT attempts to overcome those limitations. Simultaneously a strength and a limitation of CTT is its simplicity, as it does not rely on any latent variable modeling and is rather easy to understand and apply. IRT, on the other hand, is the more complex theory, but it allows more flexibility and is, when properly applied, more informative about the test and the examinees.

Methods from both theories, and combinations between them, have been successfully applied to construct and validate tests. It should be noted that both theories are, in and of themselves, insufficient for test construction and validation, since they primarily focus on reliability, and, to a lesser extent, objectivity issues. The question of test validity, on the other hand, is arguably the most important quality of a test, but there is no purely mathematical way to describe or ascertain validity - rather, it must be derived from, and validated by, theory, observation, and sometimes intuition.


\newpage

# Analysis code

In the following, the complete analysis code and its output are shown.


```{r, file='analysis.R'}
```

\newpage
# References

<div id="refs"></div>
